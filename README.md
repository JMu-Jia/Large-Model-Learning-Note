# Large Model Learning Note

## 手撕大模型

### Transformer
https://blog.csdn.net/xiaoh_7/article/details/140019530 （逐层手撕）
https://cloud.tencent.com/developer/article/1890943 （从零搭建）
- Transformer 完全基于**注意力机制**，没有递归或卷积结构，所以不具备处理序列信息的能力，其通过位置编码处理序列。

---

### LLaMa

### WebGPT


### DeepSeek
https://zhuanlan.zhihu.com/p/14953285242（论文解析）
**整体架构**：MLA + DeepSeekMoE + MTP
**训练架构**：精细化分块量化 + 提升乘累精度 + 在线量化” 的 FP8 

- **专家混合**（Mixture-of-Experts, MoE）架构自然语言生成模型
- **多头潜在注意力**（Multi-head Latent Attention, MLA）机制和**DeepSeekMoE架构**
- **多token预测** （Multi-token Prediction，MTP）

### LLM、VLM基本框架


## 微调技术 PEFT

---

### LoRA

---

### P-Tuning

---

## 知识检索增强

--- 

### RAG

---


### 向量数据库

---


### agent

---

### embedding

---


## 大模型工具

---

### DevOps工具

---

###  langchain

---
